{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2936657c-074f-4764-936a-8a68792422aa",
      "metadata": {
        "editable": true,
        "id": "2936657c-074f-4764-936a-8a68792422aa",
        "tags": []
      },
      "source": [
        "We can write a neural network with one input and one output (and three hidden units) as:\n",
        "$$\n",
        "f(x) = \\phi_{0} + \\phi_{1}\\underbrace{a(\\theta_{10} + \\theta_{11}x)}_{h_{1}} + \\phi_{2}\\underbrace{a(\\theta_{20} + \\theta_{21}x)}_{h_{2}} + \\phi_{3}\\underbrace{a(\\theta_{30} + \\theta_{31}x)}_{h_{3}}.\n",
        "$$\n",
        "\n",
        "Each $\\theta_{i0} + \\theta_{i1}x$ is a linear piece. The $a(\\ldots)$ are the non-linear functions called activations. Finally the hidden units $h_{j}$s are combined linearly by the $\\phi_{j}s$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39ebc9e3-aa81-4737-9f0b-0e728efe128c",
      "metadata": {
        "editable": true,
        "id": "39ebc9e3-aa81-4737-9f0b-0e728efe128c",
        "tags": []
      },
      "source": [
        "<img src=\"images/mlp.png\" width=800\\>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b13d174d-c3ac-45f9-9b6a-94586c5e5c3e",
      "metadata": {
        "editable": true,
        "id": "b13d174d-c3ac-45f9-9b6a-94586c5e5c3e",
        "tags": []
      },
      "source": [
        "<img src=\"images/mlp2.png\" width=1000\\>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2696b35f-2f18-4519-8134-a5593ca5c39a",
      "metadata": {
        "editable": true,
        "id": "2696b35f-2f18-4519-8134-a5593ca5c39a",
        "tags": []
      },
      "source": [
        "Neural networks are universal function approximators. Given enough neurons, a neural network with one hidden layer can approximate any continuous function.\n",
        "\n",
        "In practice, deep networks work better than wide, but shallow networks. Hence, deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04cde05e-c850-440e-979d-857eba32f7f4",
      "metadata": {
        "editable": true,
        "id": "04cde05e-c850-440e-979d-857eba32f7f4",
        "tags": []
      },
      "source": [
        "### MNIST-1D with neural networks\n",
        "\n",
        "The neural network has to take in vectors of length 40 as input, and the output is the probabilities of the image belonging to one of the ten classes.\n",
        "\n",
        "We will try a neural network with two hidden layers, each containing 100 neurons (or units).\n",
        "\n",
        "We will use ReLU as the activation function. $\\text{ReLU}(x) = 0$ if $x \\le 0$, $\\text{ReLU}(x) = x$ if $x > 0$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0580f50-c0b4-43e0-b69a-acf76f0bb2d7",
      "metadata": {
        "editable": true,
        "id": "f0580f50-c0b4-43e0-b69a-acf76f0bb2d7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "from mnist1d.data import get_dataset_args, make_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67474337-4d52-4b7f-a72e-bbd0534e5dc1",
      "metadata": {
        "editable": true,
        "id": "67474337-4d52-4b7f-a72e-bbd0534e5dc1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "defaults = get_dataset_args()\n",
        "data = make_dataset(defaults)\n",
        "x_train, y_train, x_test, y_test = (data[\"x\"], data[\"y\"], data[\"x_test\"], data[\"y_test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a19267b7-aadd-47d5-9401-dc6f05aa22ea",
      "metadata": {
        "editable": true,
        "id": "a19267b7-aadd-47d5-9401-dc6f05aa22ea",
        "outputId": "58c7ef92-bb34-44f1-ac93-a82ee4a71cbe",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train.shape, y_train.shape; np.unique(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbfa7b84-5602-4992-b09f-40ada16c2a82",
      "metadata": {
        "editable": true,
        "id": "dbfa7b84-5602-4992-b09f-40ada16c2a82",
        "outputId": "19f626ac-a493-4f94-d6fa-50091497f1a9",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((1000, 40), (1000,))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcee6b5c-1150-45c7-8805-464f15b4c297",
      "metadata": {
        "editable": true,
        "id": "fcee6b5c-1150-45c7-8805-464f15b4c297",
        "tags": []
      },
      "outputs": [],
      "source": [
        "x_train, x_test = torch.tensor(x_train, dtype=torch.float32), torch.tensor(x_test, dtype=torch.float32)\n",
        "y_train, y_test = torch.tensor(y_train), torch.tensor(y_test)\n",
        "\n",
        "train_data, test_data = TensorDataset(x_train, y_train), TensorDataset(x_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=128, shuffle=True) # shuffle=True will make the batches random\n",
        "test_loader = DataLoader(test_data, batch_size=128) # shuffle=True for train set, False for test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2753a113-9f12-44c3-9bbc-a1e1353f2a4b",
      "metadata": {
        "editable": true,
        "id": "2753a113-9f12-44c3-9bbc-a1e1353f2a4b",
        "tags": []
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b76cf32-bf2f-43bc-bf20-1c0d4e4cbeee",
      "metadata": {
        "editable": true,
        "id": "5b76cf32-bf2f-43bc-bf20-1c0d4e4cbeee",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def train(epoch, loader):\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    train_loss = 0\n",
        "    for inputs, labels in loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "\n",
        "    train_accuracy = 100 * correct / len(loader.dataset)\n",
        "    train_loss = train_loss / len(loader.sampler)\n",
        "\n",
        "    return train_accuracy, train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4804067b-1475-4c60-9e9c-01364e82d801",
      "metadata": {
        "editable": true,
        "id": "4804067b-1475-4c60-9e9c-01364e82d801",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# 1 epoch = batch size * number of batches = size of your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdbad017-4472-4fca-9454-4ec16102421a",
      "metadata": {
        "editable": true,
        "id": "cdbad017-4472-4fca-9454-4ec16102421a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def validate(epoch, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "\n",
        "    test_accuracy = 100 * correct / len(loader.dataset)\n",
        "    test_loss = test_loss / len(loader.sampler)\n",
        "\n",
        "    return test_accuracy, test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdf3565c-ab09-4763-b39c-4fc35669fc5c",
      "metadata": {
        "editable": true,
        "id": "bdf3565c-ab09-4763-b39c-4fc35669fc5c",
        "tags": []
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd11da03-4704-4db3-b0c7-c34feb93ff65",
      "metadata": {
        "editable": true,
        "id": "cd11da03-4704-4db3-b0c7-c34feb93ff65",
        "tags": []
      },
      "outputs": [],
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(40, 100), # input -> hidden 1\n",
        "    nn.BatchNorm1d(100),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100, 100), # hidden 1 -> hidden 2\n",
        "    nn.BatchNorm1d(100),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100, 10), # hidden 2 -> output\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23d8ddba-4846-4fe9-821c-81af5a5979d1",
      "metadata": {
        "editable": true,
        "id": "23d8ddba-4846-4fe9-821c-81af5a5979d1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fe71a79-0eb1-4390-8ec5-d29be38ddb23",
      "metadata": {
        "editable": true,
        "id": "6fe71a79-0eb1-4390-8ec5-d29be38ddb23",
        "outputId": "6b361aff-080c-44ac-d656-0a5b8775bd17",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100, Train Loss: 7.483e-02, Train Accuracy:  97.6%, Test Loss: 1.649e+00, Test Accuracy:  67.3%\n",
            "Epoch 200, Train Loss: 2.334e-02, Train Accuracy:  99.4%, Test Loss: 1.818e+00, Test Accuracy:  67.1%\n",
            "Epoch 300, Train Loss: 3.611e-02, Train Accuracy:  98.9%, Test Loss: 2.154e+00, Test Accuracy:  67.5%\n",
            "Epoch 400, Train Loss: 1.650e-02, Train Accuracy:  99.5%, Test Loss: 2.180e+00, Test Accuracy:  66.6%\n",
            "Epoch 500, Train Loss: 6.095e-02, Train Accuracy:  98.2%, Test Loss: 2.418e+00, Test Accuracy:  66.4%\n"
          ]
        }
      ],
      "source": [
        "epochs = 500\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_accuracy, train_loss = train(epoch, train_loader)\n",
        "    test_accuracy, test_loss = validate(epoch, test_loader)\n",
        "    if epoch % 100 == 0 or epoch == epochs:\n",
        "        print(f\"Epoch {epoch:3d},\", end=\" \")\n",
        "        print(f\"Train Loss: {train_loss:.3e}, Train Accuracy: {train_accuracy:>5.1f}%,\", end=\" \")\n",
        "        print(f\"Test Loss: {test_loss:.3e}, Test Accuracy: {test_accuracy:>5.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c0b193f-8cb4-4653-a51a-700d577ce93a",
      "metadata": {
        "editable": true,
        "id": "4c0b193f-8cb4-4653-a51a-700d577ce93a",
        "tags": []
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "fc6e6a74-82e1-4359-9895-61d6431c810d",
      "metadata": {
        "editable": true,
        "id": "fc6e6a74-82e1-4359-9895-61d6431c810d",
        "tags": []
      },
      "source": [
        "## Convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9450c787-2cc9-45c7-aecd-1ecead8da32f",
      "metadata": {
        "editable": true,
        "id": "9450c787-2cc9-45c7-aecd-1ecead8da32f",
        "tags": []
      },
      "source": [
        "The convolution between two functions is defined as<sup>1</sup>\n",
        "$$\n",
        "(f * g)(\\mathbf{z}) = \\int f(\\mathbf{x})g(\\mathbf{z}-\\mathbf{x}) \\mathrm{d}\\mathbf{x}.\n",
        "$$\n",
        "This measures the overlap between $f$ and $g$ where $g$ has been flipped and shifted by $\\mathbf{z}$.<sup>2</sup>\n",
        "\n",
        "[Visual depiction of convolution on Wikipedia](https://en.wikipedia.org/wiki/Convolution#Visual_explanation)\n",
        "\n",
        "---\n",
        "    \n",
        "1. In mathematics this operation is actually called cross-correlation, and convolution is defined slightly differently. However in ML jargon this is called convolution, and the difference between two do not matter in practice.\n",
        "2. If you are coming from signal processing, then convolution is just a filter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b2d29ad-a81e-443c-a893-98f0d7135552",
      "metadata": {
        "editable": true,
        "id": "1b2d29ad-a81e-443c-a893-98f0d7135552",
        "tags": []
      },
      "source": [
        "In machine learning we are typically interested in a discrete version of convolution.\n",
        "\n",
        "In 1D:\n",
        "$$\n",
        "(f * g)(i) = \\sum_{a} f(a)g(i - a) = \\sum_{a} f(i - a)g(a)\n",
        "$$\n",
        "\n",
        "In 2D:\n",
        "$$\n",
        "(f * g)(i, j) = \\sum_{a, b} f(a, b)g(i - a, j - b) = \\sum_{a, b} f(i - a, j - b)g(a, b)\n",
        "$$\n",
        "\n",
        "In practice, $g$ is chosen such that we can extract useful information about $f$.\n",
        "\n",
        "$g$ is also called the kernel of the convolution. For example, a *kernel size* of 3 will mean that the sum over $a$ in the 1D case will involve 3 terms."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4686aed5-ed3b-405c-88c3-c0f3cb6d2052",
      "metadata": {
        "editable": true,
        "id": "4686aed5-ed3b-405c-88c3-c0f3cb6d2052",
        "tags": []
      },
      "source": [
        "### Kernel size and padding\n",
        "\n",
        "$f(i) = x_{i}$, $g(j) = w_{j}$. If the kernel size $M$ is odd, $M = 2k + 1$:\n",
        "$$\n",
        "z_{i} = (f * g)(i) = \\sum_{a=-k}^{k} f(i-a)g(a).\n",
        "$$\n",
        "\n",
        "For example, $M = 3$, or $k = 1$:\n",
        "$$\n",
        "z_{i} = f(i-1)g(-1) + f(i)g(0) + f(i+1)g(1) = x_{i-1}w_{-1} + x_{i}w_{0} + x_{i+1}w_{1}.\n",
        "$$\n",
        "\n",
        "Often we relabel the $w_{j}$s, such that $w_{-1} \\to w_{1}$, $w_{0} \\to w_{2}$, $w_{1} \\to w_{3}$.\n",
        "\n",
        "<figure>\n",
        "    <img src=\"images/udl-10.2.png\" alt=\"Size of convolution kernels and zero padding.\">\n",
        "    <figcaption>From Understanding Deep Learning by Simon Prince</figcaption>\n",
        "</figure>\n",
        "\n",
        "To calculate $z_{1}$ and $z_{6}$, we have to use zero-padding."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8fff98d-4479-4970-a2f5-4c7a2171fe79",
      "metadata": {
        "editable": true,
        "id": "f8fff98d-4479-4970-a2f5-4c7a2171fe79",
        "tags": []
      },
      "source": [
        "### Stride and dilation\n",
        "\n",
        "Instead of calculating $z_{i}$ for every $x_{i}$, we can calculate it for every *other* (or even bigger gaps) $x_{i}$: **stride**. Stride of 2: $z_{1}$ is centered over $x_{1}$, $z_{2}$ is centered over $x_{3}$ instead of $x_{2}$.\n",
        "\n",
        "Instead of summing over consecutive $x_{i}$, we can add gaps: **dilation**.\n",
        "\n",
        "<figure>\n",
        "    <img src=\"images/udl-10.3.png\" alt=\"Stride and dilation in convolution kernels.\">\n",
        "    <figcaption>From Understanding Deep Learning by Simon Prince</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d8d5d22-8869-4ed5-b518-77f19c2a2a2f",
      "metadata": {
        "editable": true,
        "id": "6d8d5d22-8869-4ed5-b518-77f19c2a2a2f",
        "tags": []
      },
      "source": [
        "### Convolutional layers\n",
        "\n",
        "For a 1D kernel of size 3, and stride and dilation equal to 1, each unit of a convolution layer is defined as\n",
        "$$\n",
        "h_{i} = a\\left(w_{0} + \\sum_{j=1}^{3} w_{j}x_{i+j-2}\\right),\n",
        "$$\n",
        "where $a$ is an activation function.\n",
        "\n",
        "Compared to this, in the case of a linear layer, each unit is defined as\n",
        "$$\n",
        "h_{i}  = a\\left(w_{0} + \\sum_{j=1}^{L_{in}} w_{j}x_{j}\\right),\n",
        "$$\n",
        "where $L_{in}$ is the size of the input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5165d74-714f-4ff1-a7f1-46a0f9c23c3b",
      "metadata": {
        "id": "c5165d74-714f-4ff1-a7f1-46a0f9c23c3b"
      },
      "source": [
        "### Channels\n",
        "\n",
        "Usually at each layer we apply multiple convolutions to the input. These are stored in channels.\n",
        "\n",
        "Think of channels as like the depth of a layer. For example, a colored image is represented as having three channels - red, green, and blue.\n",
        "\n",
        "<figure>\n",
        "    <img src=\"images/udl-10.5.png\" alt=\"Channels in convolution layers.\">\n",
        "    <figcaption>\n",
        "         From Understanding Deep Learning by Simon Prince\n",
        "    </figcaption>\n",
        "</figure>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81526c9c-e72e-4450-adcd-72934933bf06",
      "metadata": {
        "editable": true,
        "id": "81526c9c-e72e-4450-adcd-72934933bf06",
        "tags": []
      },
      "source": [
        "## CNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7091862c-0b49-40b7-a786-964f4696db10",
      "metadata": {
        "editable": true,
        "id": "7091862c-0b49-40b7-a786-964f4696db10",
        "tags": []
      },
      "source": [
        "Some problems of neural networks with just linear layers and non-linear activations, also called fully-connected neural networks:\n",
        "\n",
        "1. They do not \"learn\" information regarding spatial relations.\n",
        "2. They have to be very large if the dataset has a lot of features.\n",
        "\n",
        "CNNs - neural networks containing convolutional layers can learn spatial relations, and are much smaller in size than fully connected neural networks with the same number of hidden layers and hidden units."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d91346ef-a0e5-4653-bb5f-d64d4abeceea",
      "metadata": {
        "editable": true,
        "id": "d91346ef-a0e5-4653-bb5f-d64d4abeceea",
        "tags": []
      },
      "source": [
        "### MNIST-1D with CNNs\n",
        "\n",
        "The inputs are 1D, so we will use 1D convolution layers. Unlike linear layers, convolution layers expect the number of input channels<sup>1</sup> and output channels, instead of the length or width of the input and the output.\n",
        "\n",
        "Typically in CNNs we increase the number of channels as we go deeper into the network to capture increasingly complex features, and learn long range spatial relations.\n",
        "\n",
        "The PyTorch 1D convolution layer will take a tensor of shape $(B, C_{in}, L_{in})$ as input and produce a tensor of shape $(B, C_{out}, L_{out})$ as output,<sup>2</sup> where $B$ is the batch size, $L_{in}$ is the length of the input (which we do not have to specify), and\n",
        "$$\n",
        "L_{out} = \\left\\lfloor\\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation} \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "1. Since MNIST-1D is grayscale, each image has a single color channel.\n",
        "2. We can think of linear layers as taking a tensor of shape $(B, H_{in})$ as input and producing a tensor of shape $(B, H_{out})$ as output. Unlike $L_{in}$ and $L_{out}$ we have to specify $H_{in}$ and $H_{out}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0401836-d6fb-4545-82b3-c1a43dc61e49",
      "metadata": {
        "editable": true,
        "id": "a0401836-d6fb-4545-82b3-c1a43dc61e49",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def l_out(l_in, padding, dilation, kernel_size, stride):\n",
        "    return math.floor((l_in + 2*padding - dilation * (kernel_size - 1) - 1) / stride + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e31a1fe8-39f8-4f8b-a41b-876032aebb6e",
      "metadata": {
        "editable": true,
        "id": "e31a1fe8-39f8-4f8b-a41b-876032aebb6e",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Conv1d, Conv2d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ad696d0-2b30-4766-ae74-bf13a662f6f9",
      "metadata": {
        "editable": true,
        "id": "2ad696d0-2b30-4766-ae74-bf13a662f6f9",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from einops.layers.torch import Rearrange, Reduce # einops enables easy manipulation of tensor shapes\n",
        "\n",
        "model = nn.Sequential(\n",
        "    Rearrange(\"b l -> b 1 l\"), # change the data shape from (B, L_in) to (B, C_in, L_in) C_in = 1\n",
        "    nn.Conv1d(in_channels=1, out_channels=25, kernel_size=5, stride=2, padding=1), # typically kernel size should be 3 or 5\n",
        "    nn.ReLU(),\n",
        "    nn.Conv1d(in_channels=25, out_channels=25, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv1d(in_channels=25, out_channels=25, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    Rearrange(\"b c l -> b (c l)\"), # change the data shape from (B, C_out, L_out) to (B, C_out * L_out) Flatten the data\n",
        "    nn.Linear(125, 10), # C_out * L_out = 125, use the `l_out` function to verify this\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c64307a1-93ed-4dc3-b729-9b9c579175fb",
      "metadata": {
        "editable": true,
        "id": "c64307a1-93ed-4dc3-b729-9b9c579175fb",
        "tags": []
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18824a38-3e08-49e2-82a8-2f37feaa4df8",
      "metadata": {
        "editable": true,
        "id": "18824a38-3e08-49e2-82a8-2f37feaa4df8",
        "outputId": "2884ad09-c15a-4e31-a883-d1f51632cdfa",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100, Train Loss: 1.339e+00, Train Accuracy: 048.4%, Test Loss: 1.355e+00, Test Accuracy 043.2%\n",
            "Epoch 200, Train Loss: 1.059e+00, Train Accuracy: 059.8%, Test Loss: 1.117e+00, Test Accuracy 054.6%\n",
            "Epoch 300, Train Loss: 7.406e-01, Train Accuracy: 073.4%, Test Loss: 8.076e-01, Test Accuracy 066.9%\n",
            "Epoch 400, Train Loss: 5.006e-01, Train Accuracy: 081.5%, Test Loss: 6.386e-01, Test Accuracy 073.6%\n",
            "Epoch 500, Train Loss: 3.526e-01, Train Accuracy: 087.7%, Test Loss: 6.065e-01, Test Accuracy 075.9%\n"
          ]
        }
      ],
      "source": [
        "epochs = 500\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_accuracy, train_loss = train(epoch, train_loader)\n",
        "    test_accuracy, test_loss = validate(epoch, test_loader)\n",
        "    if epoch % 100 == 0 or epoch == epochs:\n",
        "        print(f\"Epoch {epoch:3d},\", end=\" \")\n",
        "        print(f\"Train Loss: {train_loss:.3e}, Train Accuracy: {train_accuracy:0>5.1f}%,\", end=\" \")\n",
        "        print(f\"Test Loss: {test_loss:.3e}, Test Accuracy {test_accuracy:0>5.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fd34745-9dfe-4a94-a60b-b2050bafeb21",
      "metadata": {
        "editable": true,
        "id": "5fd34745-9dfe-4a94-a60b-b2050bafeb21",
        "tags": []
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "bf121a5d-4649-4d79-82c2-dfccc2b82676",
      "metadata": {
        "editable": true,
        "id": "bf121a5d-4649-4d79-82c2-dfccc2b82676",
        "tags": []
      },
      "source": [
        "### Batch normalization\n",
        "\n",
        "Batch normalization shifts and rescales each hidden unit such that it has zero mean and unit variance across a batch.\n",
        "\n",
        "Batch normalization makes the learning process more stable and improves convergence.\n",
        "\n",
        "It is typically applied before the activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1473daf3-3935-482a-b656-1ee32737e25d",
      "metadata": {
        "editable": true,
        "id": "1473daf3-3935-482a-b656-1ee32737e25d",
        "tags": []
      },
      "outputs": [],
      "source": [
        "model = nn.Sequential(\n",
        "    Rearrange(\"b l -> b 1 l\"), # change the data shape from (B, L_in) to (B, C_in, L_in)\n",
        "    nn.Conv1d(in_channels=1, out_channels=25, kernel_size=5, stride=2, padding=1),\n",
        "    nn.BatchNorm1d(25), # size has to match out_channel of the previous Conv1d\n",
        "    nn.ReLU(),\n",
        "    nn.Conv1d(in_channels=25, out_channels=25, kernel_size=3, stride=2, padding=1),\n",
        "    nn.BatchNorm1d(25),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv1d(in_channels=25, out_channels=25, kernel_size=3, stride=2, padding=1),\n",
        "    nn.BatchNorm1d(25),\n",
        "    nn.ReLU(),\n",
        "    Rearrange(\"b c l -> b (c l)\"), # change the data shape from (B, C_out, L_out) to (B, C_out * L_out)\n",
        "    nn.Linear(125, 10), # C_out * L_out = 125, use the `l_out` function to verify this\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d09c8ccc-d337-484d-a4c0-9299fd2f292a",
      "metadata": {
        "editable": true,
        "id": "d09c8ccc-d337-484d-a4c0-9299fd2f292a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d4ea3a8-3901-4fc7-8d87-fcf669ed09cd",
      "metadata": {
        "editable": true,
        "id": "0d4ea3a8-3901-4fc7-8d87-fcf669ed09cd",
        "tags": []
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53288156-4b1f-4f51-b332-b10dde2a60cb",
      "metadata": {
        "editable": true,
        "id": "53288156-4b1f-4f51-b332-b10dde2a60cb",
        "outputId": "5aa16477-734d-4f4a-8749-6630f0c9a910",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100, Train Loss: 2.765e-01, Train Accuracy:  93.5%, Test Loss: 4.948e-01, Test Accuracy:  82.3%\n",
            "Epoch 200, Train Loss: 6.562e-02, Train Accuracy:  99.5%, Test Loss: 2.162e-01, Test Accuracy:  93.0%\n",
            "Epoch 300, Train Loss: 3.087e-02, Train Accuracy:  99.8%, Test Loss: 2.250e-01, Test Accuracy:  92.9%\n",
            "Epoch 400, Train Loss: 1.630e-02, Train Accuracy: 100.0%, Test Loss: 1.904e-01, Test Accuracy:  93.7%\n",
            "Epoch 500, Train Loss: 1.148e-02, Train Accuracy: 100.0%, Test Loss: 2.500e-01, Test Accuracy:  92.3%\n"
          ]
        }
      ],
      "source": [
        "epochs = 500\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_accuracy, train_loss = train(epoch, train_loader)\n",
        "    test_accuracy, test_loss = validate(epoch, test_loader)\n",
        "    if epoch % 100 == 0 or epoch == epochs:\n",
        "        print(f\"Epoch {epoch:3d},\", end=\" \")\n",
        "        print(f\"Train Loss: {train_loss:.3e}, Train Accuracy: {train_accuracy:>5.1f}%,\", end=\" \")\n",
        "        print(f\"Test Loss: {test_loss:.3e}, Test Accuracy: {test_accuracy:>5.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b18ecd2-f87a-4386-a3c7-ead35e1f5a0b",
      "metadata": {
        "editable": true,
        "id": "6b18ecd2-f87a-4386-a3c7-ead35e1f5a0b",
        "tags": []
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ce60a6d7-127d-4cc3-9560-b544db30f8d4",
      "metadata": {
        "editable": true,
        "id": "ce60a6d7-127d-4cc3-9560-b544db30f8d4",
        "tags": []
      },
      "source": [
        "## Classifying CIFAR-10\n",
        "\n",
        "The [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset contains 60,000 color images in 10 classes, with 6,000 images in each class. The classes are mutually exclusive and there is no overlap between them.\n",
        "\n",
        "The images are 32 $\\times$ 32 pixels in size, and contain the three color channels. Each image is tensor of shape (3, 32, 32).\n",
        "\n",
        "The dataset is divided into 50,000 training images and 10,000 testing images.\n",
        "\n",
        "<img src=\"images/cifar10.png\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf4b8bd1-a8a7-4bdd-888e-a940a361a449",
      "metadata": {
        "editable": true,
        "id": "bf4b8bd1-a8a7-4bdd-888e-a940a361a449",
        "outputId": "2b386550-7cbf-46ed-b8c6-16630635434f",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 170M/170M [00:02<00:00, 63.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torchvision.datasets as datasets\n",
        "\n",
        "train_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
        "test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20819960-0920-433e-b05e-d8ac0f217ecf",
      "metadata": {
        "id": "20819960-0920-433e-b05e-d8ac0f217ecf",
        "outputId": "b6116c98-209e-4330-bf80-57da38fbe491"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<PIL.Image.Image image mode=RGB size=32x32>, 6)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0d8d8cd-4425-4d68-91ac-dde595df9e3b",
      "metadata": {
        "editable": true,
        "id": "d0d8d8cd-4425-4d68-91ac-dde595df9e3b",
        "outputId": "b4747def-2d5e-4e0e-f91e-a28a09793be2",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f29e1613a90>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 80x80 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(1, 1), dpi=80)\n",
        "plt.imshow(train_data[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba901b42-3b54-4342-b318-44cdb242e46c",
      "metadata": {
        "editable": true,
        "id": "ba901b42-3b54-4342-b318-44cdb242e46c",
        "tags": []
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "435ad09a-a02e-4429-82fe-f557a0639ba1",
      "metadata": {
        "editable": true,
        "id": "435ad09a-a02e-4429-82fe-f557a0639ba1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms.v2 as v2\n",
        "\n",
        "transform = v2.Compose([\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True), # convert to tensor\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # scale the tensor, so that the values lie between 0 & 1\n",
        "    # mean and std are derived from the IMAGENET data\n",
        "])\n",
        "\n",
        "train_data = datasets.CIFAR10(root=\"./data\", train=True, download=False, transform=transform)\n",
        "test_data = datasets.CIFAR10(root='./data', train=False, download=False, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a2ce6a6-289d-48f7-876f-61609908c1cb",
      "metadata": {
        "editable": true,
        "id": "4a2ce6a6-289d-48f7-876f-61609908c1cb",
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5831ba20-aa03-4c05-8c0b-457691787788",
      "metadata": {
        "editable": true,
        "id": "5831ba20-aa03-4c05-8c0b-457691787788",
        "outputId": "36d49ebb-8069-46fc-8704-15d077f9ad92",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 3, 32, 32])"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(train_loader))[0].shape # batch, channel, height, width"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28fc6a6b-0b2e-4637-b81a-1cb6cd956570",
      "metadata": {
        "editable": true,
        "id": "28fc6a6b-0b2e-4637-b81a-1cb6cd956570",
        "tags": []
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2d77efc-d55c-479e-a908-ef839b9ac81a",
      "metadata": {
        "editable": true,
        "id": "e2d77efc-d55c-479e-a908-ef839b9ac81a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class CIFARClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(CIFARClassifier, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2), # MaxPool2d(2) will decrease the image image height and width by 2, output shape = (B, 32, 16, 16)\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2), # output shape = (B, 64, 8, 8)\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            Rearrange(\"b c h w -> b (c h w)\"), # flatten the image\n",
        "            nn.Linear(64 * 8 * 8, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f36b4179-a005-4dbe-ab05-29f3fbb43093",
      "metadata": {
        "editable": true,
        "id": "f36b4179-a005-4dbe-ab05-29f3fbb43093",
        "tags": []
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1571da90-af09-466e-8594-b2b842c2a393",
      "metadata": {
        "editable": true,
        "id": "1571da90-af09-466e-8594-b2b842c2a393",
        "outputId": "1f3fa263-8f9c-4cf4-b362-2149e448d252",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CIFARClassifier(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU()\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Rearrange('b c h w -> b (c h w)')\n",
            "    (1): Linear(in_features=4096, out_features=128, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=128, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = CIFARClassifier()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b2be4a1-54ec-4f89-9cfc-66f606a27867",
      "metadata": {
        "editable": true,
        "id": "0b2be4a1-54ec-4f89-9cfc-66f606a27867",
        "tags": []
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bad75a2-e564-4a3e-8d3a-53cb52fa2ff0",
      "metadata": {
        "editable": true,
        "id": "1bad75a2-e564-4a3e-8d3a-53cb52fa2ff0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# model = nn.Sequential(\n",
        "#     nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "#     nn.BatchNorm2d(32),\n",
        "#     nn.ReLU(),\n",
        "#     nn.MaxPool2d(kernel_size=2),\n",
        "#     nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "#     nn.BatchNorm2d(64),\n",
        "#     nn.ReLU(),\n",
        "#     nn.MaxPool2d(kernel_size=2), # ouput shape = (B, 64, 8, 8)\n",
        "#     Rearrange(\"b c h w -> b (c h w)\"),\n",
        "#     nn.Linear(64 * 8 * 8, 128),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Linear(128, num_classes),\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44f7b3b9-4dd1-4a5f-8463-080cccdf8b8c",
      "metadata": {
        "editable": true,
        "id": "44f7b3b9-4dd1-4a5f-8463-080cccdf8b8c",
        "tags": []
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e6cc6af-c63b-431d-a9ce-26bccee9ea66",
      "metadata": {
        "editable": true,
        "id": "0e6cc6af-c63b-431d-a9ce-26bccee9ea66",
        "tags": []
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e8b6abe-7e74-4dbf-81c1-0fb864dd2a5a",
      "metadata": {
        "editable": true,
        "id": "8e8b6abe-7e74-4dbf-81c1-0fb864dd2a5a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def train(epoch, loader):\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    train_loss = 0\n",
        "    for inputs, labels in loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "\n",
        "    train_accuracy = 100 * correct / len(loader.dataset)\n",
        "    train_loss = train_loss / len(loader.sampler)\n",
        "\n",
        "    return train_accuracy, train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed6912f5-57e4-464e-be80-417a74473c0c",
      "metadata": {
        "editable": true,
        "id": "ed6912f5-57e4-464e-be80-417a74473c0c",
        "tags": []
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48528d06-fc2e-45c0-92d2-6d7ca8319057",
      "metadata": {
        "editable": true,
        "id": "48528d06-fc2e-45c0-92d2-6d7ca8319057",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def validate(epoch, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "\n",
        "    test_accuracy = 100 * correct / len(loader.dataset)\n",
        "    test_loss = test_loss / len(loader.sampler)\n",
        "\n",
        "    return test_accuracy, test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16783baa-5129-41d1-a5f5-0768dc4aba61",
      "metadata": {
        "editable": true,
        "id": "16783baa-5129-41d1-a5f5-0768dc4aba61",
        "tags": []
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05573b82-5446-4ee0-a162-aa995c14cc34",
      "metadata": {
        "editable": true,
        "id": "05573b82-5446-4ee0-a162-aa995c14cc34",
        "outputId": "1bcac047-71f3-4815-9144-6062d79156a6",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1, Train Loss: 1.259e+00, Train Accuracy:  54.6%, Test Loss: 9.807e-01, Test Accuracy:  65.6%\n",
            "Epoch   2, Train Loss: 9.229e-01, Train Accuracy:  67.6%, Test Loss: 9.378e-01, Test Accuracy:  66.6%\n",
            "Epoch   3, Train Loss: 8.057e-01, Train Accuracy:  71.7%, Test Loss: 8.398e-01, Test Accuracy:  71.0%\n",
            "Epoch   4, Train Loss: 7.188e-01, Train Accuracy:  75.0%, Test Loss: 8.150e-01, Test Accuracy:  71.5%\n",
            "Epoch   5, Train Loss: 6.521e-01, Train Accuracy:  77.1%, Test Loss: 8.133e-01, Test Accuracy:  72.3%\n"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_accuracy, train_loss = train(epoch, train_loader)\n",
        "    test_accuracy, test_loss = validate(epoch, test_loader)\n",
        "    if epoch % 1 == 0 or epoch == epochs:\n",
        "        print(f\"Epoch {epoch:3d},\", end=\" \")\n",
        "        print(f\"Train Loss: {train_loss:.3e}, Train Accuracy: {train_accuracy:>5.1f}%,\", end=\" \")\n",
        "        print(f\"Test Loss: {test_loss:.3e}, Test Accuracy: {test_accuracy:>5.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13a287c1-d45b-4bb8-86ac-87dbfb276f5a",
      "metadata": {
        "id": "13a287c1-d45b-4bb8-86ac-87dbfb276f5a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4a2f0beb-6286-4236-a1a7-fe95181c6904",
      "metadata": {
        "editable": true,
        "id": "4a2f0beb-6286-4236-a1a7-fe95181c6904",
        "tags": []
      },
      "source": [
        "## References\n",
        "\n",
        "1. Understanding Deep Learning\n",
        "2. Dive into Deep Learning\n",
        "3. [Stanford CS231n notes](https://cs231n.github.io/convolutional-networks/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0da6b76c-abd7-445d-9aaa-2503bb20a18f",
      "metadata": {
        "id": "0da6b76c-abd7-445d-9aaa-2503bb20a18f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
